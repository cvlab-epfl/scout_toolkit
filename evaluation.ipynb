{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scout Dataset Tracking Evaluation Examples\n",
    "\n",
    "This notebook demonstrates two approaches for computing tracking metrics on the Scout dataset:\n",
    "\n",
    "1. **3D Tracking Metrics**: Using `compute_mot_metric` for evaluation in 3D world coordinates\n",
    "2. **Monocular Tracking Metrics**: Using `MOTMetricEvaluator` for standard MOT evaluation\n",
    "\n",
    "Both methods work by:\n",
    "- Loading a sequence of ground truth annotations\n",
    "- Simulating predictions by randomly modifying the ground truth\n",
    "- Computing tracking metrics between GT and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "if not hasattr(np, 'float'):\n",
    "    np.float = float\n",
    "if not hasattr(np, 'int'):\n",
    "    np.int = int\n",
    "if not hasattr(np, 'bool'):\n",
    "    np.bool = bool\n",
    "if not hasattr(np, 'object'):\n",
    "    np.object = object\n",
    "if not hasattr(np, 'str'):\n",
    "    np.str = str\n",
    "    \n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "import motmetrics as mm\n",
    "\n",
    "from src.loading import Individual\n",
    "from src.metric import (MOTMetricEvaluator, compute_mot_metric, convert_scout_to_tensors, \n",
    "                        scout_to_dataframe, simulate_predictions, validate_unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "rootdir = '/path/to/scout/'\n",
    "sequence_id = 1\n",
    "camera_id = 'cam_2'\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Scout Dataset\n",
    "indiv = Individual(rootdir, sequence=sequence_id)\n",
    "print(f\"Loaded sequence {sequence_id} from {rootdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Ground Truth Sequence\n",
    "# Load annotations for multiple frames to create a sequence\n",
    "start_frame = 80\n",
    "end_frame = 90\n",
    "frame_range = range(start_frame, end_frame + 1)\n",
    "\n",
    "gt_sequence = {}\n",
    "for frame_id in frame_range:\n",
    "    try:\n",
    "        ann = indiv.retrieve(frame_id, camera_id)\n",
    "        if ann:  # Only add frames with annotations\n",
    "            gt_sequence[frame_id] = ann\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load frame {frame_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Loaded {len(gt_sequence)} frames with annotations\")\n",
    "print(f\"Frame range: {min(gt_sequence.keys())} to {max(gt_sequence.keys())}\")\n",
    "\n",
    "# Show first frame as example\n",
    "if gt_sequence:\n",
    "    first_frame = min(gt_sequence.keys())\n",
    "    print(f\"\\nExample annotation for frame {first_frame}:\")\n",
    "    print(f\"Number of detections: {len(gt_sequence[first_frame])}\")\n",
    "    print(f\"First detection: {gt_sequence[first_frame][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Simulated Predictions\n",
    "# All simulation functions are now imported from src.metric\n",
    "\n",
    "# Generate predictions\n",
    "pred_sequence = simulate_predictions(gt_sequence)\n",
    "\n",
    "# Validate that predictions have unique IDs per frame\n",
    "print(\"Validating unique IDs in predictions...\")\n",
    "if validate_unique_ids(pred_sequence):\n",
    "    print(\"✓ All track IDs are unique within each frame\")\n",
    "else:\n",
    "    print(\"✗ Found duplicate track IDs - regenerating predictions...\")\n",
    "    pred_sequence = simulate_predictions(gt_sequence)\n",
    "\n",
    "print(\"\\nGround Truth vs Predictions Summary:\")\n",
    "for frame_id in sorted(gt_sequence.keys()):\n",
    "    gt_count = len(gt_sequence[frame_id])\n",
    "    pred_count = len(pred_sequence.get(frame_id, []))\n",
    "    print(f\"Frame {frame_id}: GT={gt_count}, Pred={pred_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# USE CASE 1: 3D Tracking Metrics using compute_mot_metric\n",
    "# ================================================================================================\n",
    "\n",
    "## Convert Scout annotations to the format expected by compute_mot_metric\n",
    "# scout_to_dataframe function is now imported from src.metric\n",
    "\n",
    "# Convert ground truth and predictions to dataframe format\n",
    "print(\"Converting data to dataframe format for 3D tracking evaluation...\")\n",
    "\n",
    "# Using world coordinates for 3D evaluation\n",
    "gt_df_3d = scout_to_dataframe(gt_sequence, use_world_coords=True)\n",
    "pred_df_3d = scout_to_dataframe(pred_sequence, use_world_coords=True)\n",
    "\n",
    "print(f\"Ground Truth DataFrame shape: {gt_df_3d.shape}\")\n",
    "print(f\"Predictions DataFrame shape: {pred_df_3d.shape}\")\n",
    "print(f\"\\nGround Truth DataFrame head:\")\n",
    "print(gt_df_3d.head())\n",
    "print(f\"\\nPredictions DataFrame head:\")\n",
    "print(pred_df_3d.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute 3D Tracking Metrics using compute_mot_metric\n",
    "\n",
    "# Define distance threshold for matching in meters (world coordinates)\n",
    "distance_threshold_3d = 1.0  # 1 meter threshold for 3D matching\n",
    "\n",
    "# Count ground truth objects for metrics computation\n",
    "nb_gt = len(gt_df_3d)\n",
    "\n",
    "print(f\"Computing 3D tracking metrics with distance threshold: {distance_threshold_3d}m\")\n",
    "print(f\"Number of ground truth detections: {nb_gt}\")\n",
    "\n",
    "# Compute metrics\n",
    "if nb_gt > 0:\n",
    "    metrics_3d = compute_mot_metric(gt_df_3d, pred_df_3d, distance_threshold_3d, nb_gt)\n",
    "    \n",
    "    if metrics_3d:\n",
    "        print(\"\\n=== 3D Tracking Metrics Results ===\")\n",
    "        print(f\"MOTA (Multiple Object Tracking Accuracy): {metrics_3d.get('mota', 'N/A'):.3f}\")\n",
    "        print(f\"MOTP (Multiple Object Tracking Precision): {metrics_3d.get('motp', 'N/A'):.3f}\")\n",
    "        print(f\"IDF1 (ID F1 Score): {metrics_3d.get('idf1', 'N/A'):.3f}\")\n",
    "        print(f\"Number of Switches: {metrics_3d.get('num_switches', 'N/A')}\")\n",
    "        print(f\"Number of Fragmentations: {metrics_3d.get('num_fragmentations', 'N/A')}\")\n",
    "        print(f\"Number of False Positives: {metrics_3d.get('num_false_positives', 'N/A')}\")\n",
    "        print(f\"Number of Misses: {metrics_3d.get('num_misses', 'N/A')}\")\n",
    "        print(f\"Precision: {metrics_3d.get('precision', 'N/A'):.3f}\")\n",
    "        print(f\"Recall: {metrics_3d.get('recall', 'N/A'):.3f}\")\n",
    "        \n",
    "        print(f\"\\nFull metrics dictionary:\")\n",
    "        for key, value in metrics_3d.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(\"Failed to compute metrics\")\n",
    "else:\n",
    "    print(\"No ground truth data available for metrics computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# USE CASE 2: Monocular Tracking Metrics using MOTMetricEvaluator  \n",
    "# ================================================================================================\n",
    "\n",
    "## Convert Scout data to format expected by MOTMetricEvaluator\n",
    "\n",
    "print(\"Converting data for MOTMetricEvaluator...\")\n",
    "\n",
    "# Convert data using the cleaner helper function\n",
    "gt_dict, pred_bboxes, pred_world_points, pred_timestamps, pred_ids = convert_scout_to_tensors(gt_sequence, pred_sequence)\n",
    "\n",
    "print(f\"Ground truth dictionary has {len(gt_dict)} timestamps\")\n",
    "print(f\"Prediction tensors: bboxes {pred_bboxes.shape}, world_points {pred_world_points.shape}\")\n",
    "print(f\"Prediction IDs: {len(pred_ids)} track assignments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run MOTMetricEvaluator\n",
    "\n",
    "# trackeval library might conflict with recent data type updates of numpy. Replace np.float with float directly in trackeval code should fix it.\n",
    "\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = MOTMetricEvaluator(interpolate_missing_detections=False)\n",
    "\n",
    "# Save sequence data  \n",
    "print(\"Saving sequence data for MOT evaluation...\")\n",
    "gt_world_data, pred_world_data = evaluator.save_sequence_data(\n",
    "    pred_bboxes=pred_bboxes,\n",
    "    pred_world_points=pred_world_points,\n",
    "    pred_timestamps=pred_timestamps,\n",
    "    pred_ids=pred_ids, \n",
    "    gt_dict=gt_dict,\n",
    "    dset_name=\"SCOUT\",\n",
    "    sequence=\"test_sequence\"\n",
    ")\n",
    "\n",
    "print(\"Computing MOT metrics...\")\n",
    "\n",
    "# Compute metrics using trackeval\n",
    "sequence_metrics = evaluator.compute_metrics(\"SCOUT\")\n",
    "\n",
    "print(\"\\n=== MOT Evaluation Results (using trackeval) ===\")\n",
    "\n",
    "# Display key metrics\n",
    "key_metrics = ['HOTA', 'DetA', 'AssA', 'MOTA', 'MOTP', 'IDF1', 'IDSW']\n",
    "\n",
    "for metric in key_metrics:\n",
    "    combined_key = f'combined_{metric}'\n",
    "    if combined_key in sequence_metrics:\n",
    "        print(f\"{metric}: {sequence_metrics[combined_key]:.3f}\")\n",
    "\n",
    "print(f\"\\nFull metrics available:\")\n",
    "for key, value in sequence_metrics.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
